{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4 Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "For this part of the lab we’ll work through the mbta.xlsx data. The Massachusetts Bay Transportation Authority(“MBTA”) manages America’s oldest subway, as well as Greater Boston’s commuter rail, ferry, and bus systems.It’s your fi rst day on the job as the T’s data analyst and you’ve been tasked with analyzing average ridershipthrough time. Complete the following data cleaning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the data. Note, you'll need to skip the first row and have the first column be used as the index. Checkout the read_excel()\n",
    "docs to figure out how to do this. The resulting DataFrame should contain 11 rowsand 59 columns and look similar to the below DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>2007-01</th>\n",
       "      <th>2007-02</th>\n",
       "      <th>2007-03</th>\n",
       "      <th>2007-04</th>\n",
       "      <th>2007-05</th>\n",
       "      <th>2007-06</th>\n",
       "      <th>2007-07</th>\n",
       "      <th>2007-08</th>\n",
       "      <th>2007-09</th>\n",
       "      <th>...</th>\n",
       "      <th>2011-01</th>\n",
       "      <th>2011-02</th>\n",
       "      <th>2011-03</th>\n",
       "      <th>2011-04</th>\n",
       "      <th>2011-05</th>\n",
       "      <th>2011-06</th>\n",
       "      <th>2011-07</th>\n",
       "      <th>2011-08</th>\n",
       "      <th>2011-09</th>\n",
       "      <th>2011-10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All Modes by Qtr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1187.653</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1245.959</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1256.571</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1223.452</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1302.414</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1290.549</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boat</td>\n",
       "      <td>4.000</td>\n",
       "      <td>3.600</td>\n",
       "      <td>40.000</td>\n",
       "      <td>4.300</td>\n",
       "      <td>4.900</td>\n",
       "      <td>5.800</td>\n",
       "      <td>6.521</td>\n",
       "      <td>6.572</td>\n",
       "      <td>5.469</td>\n",
       "      <td>...</td>\n",
       "      <td>3.140</td>\n",
       "      <td>3.284</td>\n",
       "      <td>3.674</td>\n",
       "      <td>4.251</td>\n",
       "      <td>4.431</td>\n",
       "      <td>5.474</td>\n",
       "      <td>6.581</td>\n",
       "      <td>6.733</td>\n",
       "      <td>5.003</td>\n",
       "      <td>4.484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bus</td>\n",
       "      <td>335.819</td>\n",
       "      <td>338.675</td>\n",
       "      <td>339.867</td>\n",
       "      <td>352.162</td>\n",
       "      <td>354.367</td>\n",
       "      <td>350.543</td>\n",
       "      <td>357.519</td>\n",
       "      <td>355.479</td>\n",
       "      <td>372.598</td>\n",
       "      <td>...</td>\n",
       "      <td>334.958</td>\n",
       "      <td>346.234</td>\n",
       "      <td>380.399</td>\n",
       "      <td>380.446</td>\n",
       "      <td>385.289</td>\n",
       "      <td>376.317</td>\n",
       "      <td>361.585</td>\n",
       "      <td>353.793</td>\n",
       "      <td>388.271</td>\n",
       "      <td>398.456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Commuter Rail</td>\n",
       "      <td>142.200</td>\n",
       "      <td>138.500</td>\n",
       "      <td>137.700</td>\n",
       "      <td>139.500</td>\n",
       "      <td>139.000</td>\n",
       "      <td>143.000</td>\n",
       "      <td>142.391</td>\n",
       "      <td>142.364</td>\n",
       "      <td>143.051</td>\n",
       "      <td>...</td>\n",
       "      <td>128.396</td>\n",
       "      <td>125.463</td>\n",
       "      <td>134.374</td>\n",
       "      <td>134.169</td>\n",
       "      <td>136.140</td>\n",
       "      <td>135.581</td>\n",
       "      <td>132.410</td>\n",
       "      <td>130.616</td>\n",
       "      <td>136.901</td>\n",
       "      <td>128.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Heavy Rail</td>\n",
       "      <td>435.294</td>\n",
       "      <td>448.271</td>\n",
       "      <td>458.583</td>\n",
       "      <td>472.201</td>\n",
       "      <td>474.579</td>\n",
       "      <td>477.032</td>\n",
       "      <td>471.735</td>\n",
       "      <td>461.605</td>\n",
       "      <td>499.566</td>\n",
       "      <td>...</td>\n",
       "      <td>468.418</td>\n",
       "      <td>504.068</td>\n",
       "      <td>516.730</td>\n",
       "      <td>528.631</td>\n",
       "      <td>528.122</td>\n",
       "      <td>529.528</td>\n",
       "      <td>532.888</td>\n",
       "      <td>508.145</td>\n",
       "      <td>550.137</td>\n",
       "      <td>554.932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Light Rail</td>\n",
       "      <td>227.231</td>\n",
       "      <td>240.262</td>\n",
       "      <td>241.444</td>\n",
       "      <td>255.557</td>\n",
       "      <td>248.262</td>\n",
       "      <td>246.108</td>\n",
       "      <td>243.286</td>\n",
       "      <td>234.907</td>\n",
       "      <td>265.748</td>\n",
       "      <td>...</td>\n",
       "      <td>198.450</td>\n",
       "      <td>219.886</td>\n",
       "      <td>227.935</td>\n",
       "      <td>242.280</td>\n",
       "      <td>225.776</td>\n",
       "      <td>221.865</td>\n",
       "      <td>231.010</td>\n",
       "      <td>220.164</td>\n",
       "      <td>244.949</td>\n",
       "      <td>237.768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pct Chg / Yr</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Private Bus</td>\n",
       "      <td>4.772</td>\n",
       "      <td>4.417</td>\n",
       "      <td>4.574</td>\n",
       "      <td>4.542</td>\n",
       "      <td>4.768</td>\n",
       "      <td>4.722</td>\n",
       "      <td>3.936</td>\n",
       "      <td>3.946</td>\n",
       "      <td>4.329</td>\n",
       "      <td>...</td>\n",
       "      <td>2.213</td>\n",
       "      <td>2.570</td>\n",
       "      <td>2.559</td>\n",
       "      <td>2.762</td>\n",
       "      <td>2.776</td>\n",
       "      <td>2.815</td>\n",
       "      <td>2.671</td>\n",
       "      <td>2.655</td>\n",
       "      <td>2.843</td>\n",
       "      <td>2.967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RIDE</td>\n",
       "      <td>4.900</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.500</td>\n",
       "      <td>5.400</td>\n",
       "      <td>5.400</td>\n",
       "      <td>5.600</td>\n",
       "      <td>5.253</td>\n",
       "      <td>5.308</td>\n",
       "      <td>5.609</td>\n",
       "      <td>...</td>\n",
       "      <td>6.735</td>\n",
       "      <td>7.463</td>\n",
       "      <td>8.387</td>\n",
       "      <td>8.145</td>\n",
       "      <td>8.059</td>\n",
       "      <td>8.377</td>\n",
       "      <td>7.902</td>\n",
       "      <td>8.071</td>\n",
       "      <td>8.318</td>\n",
       "      <td>8.598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Trackless Trolley</td>\n",
       "      <td>12.757</td>\n",
       "      <td>12.913</td>\n",
       "      <td>13.057</td>\n",
       "      <td>13.444</td>\n",
       "      <td>13.479</td>\n",
       "      <td>13.323</td>\n",
       "      <td>13.311</td>\n",
       "      <td>13.142</td>\n",
       "      <td>14.393</td>\n",
       "      <td>...</td>\n",
       "      <td>11.104</td>\n",
       "      <td>11.695</td>\n",
       "      <td>12.601</td>\n",
       "      <td>12.599</td>\n",
       "      <td>12.291</td>\n",
       "      <td>12.128</td>\n",
       "      <td>11.060</td>\n",
       "      <td>11.091</td>\n",
       "      <td>12.332</td>\n",
       "      <td>12.297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TOTAL</td>\n",
       "      <td>1166.974</td>\n",
       "      <td>1191.639</td>\n",
       "      <td>1204.725</td>\n",
       "      <td>1247.105</td>\n",
       "      <td>1244.755</td>\n",
       "      <td>1246.129</td>\n",
       "      <td>1243.952</td>\n",
       "      <td>1223.323</td>\n",
       "      <td>1310.764</td>\n",
       "      <td>...</td>\n",
       "      <td>1153.413</td>\n",
       "      <td>1220.663</td>\n",
       "      <td>1286.660</td>\n",
       "      <td>1313.283</td>\n",
       "      <td>1302.884</td>\n",
       "      <td>1292.085</td>\n",
       "      <td>1286.107</td>\n",
       "      <td>1241.268</td>\n",
       "      <td>1348.754</td>\n",
       "      <td>1348.222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mode   2007-01   2007-02   2007-03   2007-04   2007-05  \\\n",
       "0    All Modes by Qtr       NaN       NaN  1187.653       NaN       NaN   \n",
       "1                Boat     4.000     3.600    40.000     4.300     4.900   \n",
       "2                 Bus   335.819   338.675   339.867   352.162   354.367   \n",
       "3       Commuter Rail   142.200   138.500   137.700   139.500   139.000   \n",
       "4          Heavy Rail   435.294   448.271   458.583   472.201   474.579   \n",
       "5          Light Rail   227.231   240.262   241.444   255.557   248.262   \n",
       "6        Pct Chg / Yr     0.020    -0.040     0.114    -0.002     0.049   \n",
       "7         Private Bus     4.772     4.417     4.574     4.542     4.768   \n",
       "8                RIDE     4.900     5.000     5.500     5.400     5.400   \n",
       "9   Trackless Trolley    12.757    12.913    13.057    13.444    13.479   \n",
       "10              TOTAL  1166.974  1191.639  1204.725  1247.105  1244.755   \n",
       "\n",
       "     2007-06   2007-07   2007-08   2007-09  ...   2011-01   2011-02   2011-03  \\\n",
       "0   1245.959       NaN       NaN  1256.571  ...       NaN       NaN  1223.452   \n",
       "1      5.800     6.521     6.572     5.469  ...     3.140     3.284     3.674   \n",
       "2    350.543   357.519   355.479   372.598  ...   334.958   346.234   380.399   \n",
       "3    143.000   142.391   142.364   143.051  ...   128.396   125.463   134.374   \n",
       "4    477.032   471.735   461.605   499.566  ...   468.418   504.068   516.730   \n",
       "5    246.108   243.286   234.907   265.748  ...   198.450   219.886   227.935   \n",
       "6      0.096    -0.037     0.004    -0.007  ...    -0.028     0.008     0.050   \n",
       "7      4.722     3.936     3.946     4.329  ...     2.213     2.570     2.559   \n",
       "8      5.600     5.253     5.308     5.609  ...     6.735     7.463     8.387   \n",
       "9     13.323    13.311    13.142    14.393  ...    11.104    11.695    12.601   \n",
       "10  1246.129  1243.952  1223.323  1310.764  ...  1153.413  1220.663  1286.660   \n",
       "\n",
       "     2011-04   2011-05   2011-06   2011-07   2011-08   2011-09   2011-10  \n",
       "0        NaN       NaN  1302.414       NaN       NaN  1290.549       NaN  \n",
       "1      4.251     4.431     5.474     6.581     6.733     5.003     4.484  \n",
       "2    380.446   385.289   376.317   361.585   353.793   388.271   398.456  \n",
       "3    134.169   136.140   135.581   132.410   130.616   136.901   128.720  \n",
       "4    528.631   528.122   529.528   532.888   508.145   550.137   554.932  \n",
       "5    242.280   225.776   221.865   231.010   220.164   244.949   237.768  \n",
       "6      0.036     0.050     0.054     0.067     0.052     0.043     0.032  \n",
       "7      2.762     2.776     2.815     2.671     2.655     2.843     2.967  \n",
       "8      8.145     8.059     8.377     7.902     8.071     8.318     8.598  \n",
       "9     12.599    12.291    12.128    11.060    11.091    12.332    12.297  \n",
       "10  1313.283  1302.884  1292.085  1286.107  1241.268  1348.754  1348.222  \n",
       "\n",
       "[11 rows x 59 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "mbta_1 = pd.read_excel('C:/Users/dell/OneDrive/Documents/Study_Fall_2022/Statistical_Computing/Module_4/mbta.xlsx', skiprows=1)\n",
    "mbta_1\n",
    "\n",
    "mbta_2 = mbta_1.iloc[:,1:]\n",
    "mbta_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. How many missing values are in each column? How many missing values are there in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column are :  mode       0\n",
      "2007-01    1\n",
      "2007-02    1\n",
      "2007-03    0\n",
      "2007-04    1\n",
      "2007-05    1\n",
      "2007-06    0\n",
      "2007-07    1\n",
      "2007-08    1\n",
      "2007-09    0\n",
      "2007-10    1\n",
      "2007-11    1\n",
      "2007-12    0\n",
      "2008-01    1\n",
      "2008-02    1\n",
      "2008-03    0\n",
      "2008-04    1\n",
      "2008-05    1\n",
      "2008-06    0\n",
      "2008-07    1\n",
      "2008-08    1\n",
      "2008-09    0\n",
      "2008-10    1\n",
      "2008-11    1\n",
      "2008-12    0\n",
      "2009-01    1\n",
      "2009-02    1\n",
      "2009-03    0\n",
      "2009-04    1\n",
      "2009-05    1\n",
      "2009-06    0\n",
      "2009-07    1\n",
      "2009-08    1\n",
      "2009-09    0\n",
      "2009-10    1\n",
      "2009-11    1\n",
      "2009-12    0\n",
      "2010-01    1\n",
      "2010-02    1\n",
      "2010-03    0\n",
      "2010-04    1\n",
      "2010-05    1\n",
      "2010-06    0\n",
      "2010-07    1\n",
      "2010-08    1\n",
      "2010-09    0\n",
      "2010-10    1\n",
      "2010-11    1\n",
      "2010-12    0\n",
      "2011-01    1\n",
      "2011-02    1\n",
      "2011-03    0\n",
      "2011-04    1\n",
      "2011-05    1\n",
      "2011-06    0\n",
      "2011-07    1\n",
      "2011-08    1\n",
      "2011-09    0\n",
      "2011-10    1\n",
      "dtype: int64\n",
      "The total missing values :  39\n"
     ]
    }
   ],
   "source": [
    "#Missing Values are not present\n",
    "\t\n",
    "print(\"Missing values in each column are : \" ,mbta_2.isna().sum())\n",
    "print(\"The total missing values : \" ,mbta_2.isnull().sum().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. It appears that the data are organized with observations stored as columns rather than as rows. You can fi xthat. First, though, you can address the missing data. All of the NA values are stored in the fi rst row. This rowreally belongs in a diff erent data frame; it is a quarterly average of weekday MBTA ridership. Since this data settracks monthly average ridership, you’ll remove that row. Similarly, the 7th row (Pct Chg / Yr) and the 11th row(TOTAL) are not really observations as much as they are analysis. Go ahead and remove the 7th and 11th rowsas well. After removing the fi rst, seventh, and eleventh rows, what are the dimensions of the new DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                mode  2007-01  2007-02  2007-03  2007-04  2007-05  2007-06  \\\n",
      "1               Boat    4.000    3.600   40.000    4.300    4.900    5.800   \n",
      "2                Bus  335.819  338.675  339.867  352.162  354.367  350.543   \n",
      "3      Commuter Rail  142.200  138.500  137.700  139.500  139.000  143.000   \n",
      "4         Heavy Rail  435.294  448.271  458.583  472.201  474.579  477.032   \n",
      "5         Light Rail  227.231  240.262  241.444  255.557  248.262  246.108   \n",
      "7        Private Bus    4.772    4.417    4.574    4.542    4.768    4.722   \n",
      "8               RIDE    4.900    5.000    5.500    5.400    5.400    5.600   \n",
      "9  Trackless Trolley   12.757   12.913   13.057   13.444   13.479   13.323   \n",
      "\n",
      "   2007-07  2007-08  2007-09  ...  2011-01  2011-02  2011-03  2011-04  \\\n",
      "1    6.521    6.572    5.469  ...    3.140    3.284    3.674    4.251   \n",
      "2  357.519  355.479  372.598  ...  334.958  346.234  380.399  380.446   \n",
      "3  142.391  142.364  143.051  ...  128.396  125.463  134.374  134.169   \n",
      "4  471.735  461.605  499.566  ...  468.418  504.068  516.730  528.631   \n",
      "5  243.286  234.907  265.748  ...  198.450  219.886  227.935  242.280   \n",
      "7    3.936    3.946    4.329  ...    2.213    2.570    2.559    2.762   \n",
      "8    5.253    5.308    5.609  ...    6.735    7.463    8.387    8.145   \n",
      "9   13.311   13.142   14.393  ...   11.104   11.695   12.601   12.599   \n",
      "\n",
      "   2011-05  2011-06  2011-07  2011-08  2011-09  2011-10  \n",
      "1    4.431    5.474    6.581    6.733    5.003    4.484  \n",
      "2  385.289  376.317  361.585  353.793  388.271  398.456  \n",
      "3  136.140  135.581  132.410  130.616  136.901  128.720  \n",
      "4  528.122  529.528  532.888  508.145  550.137  554.932  \n",
      "5  225.776  221.865  231.010  220.164  244.949  237.768  \n",
      "7    2.776    2.815    2.671    2.655    2.843    2.967  \n",
      "8    8.059    8.377    7.902    8.071    8.318    8.598  \n",
      "9   12.291   12.128   11.060   11.091   12.332   12.297  \n",
      "\n",
      "[8 rows x 59 columns]\n",
      "After removing the fi rst, seventh, and eleventh rows, the dimensions of the new DataFrame are (8, 59)\n"
     ]
    }
   ],
   "source": [
    "mbta_2\n",
    "mbta_2_drop = mbta_2.drop([mbta_2.index[0],mbta_2.index[6],mbta_2.index[10]])\n",
    "print(mbta_2_drop)\n",
    "print('After removing the fi rst, seventh, and eleventh rows, the dimensions of the new DataFrame are',mbta_2_drop.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. In this data, variables are stored in rows instead of columns. The different modes of transportation (commuterrail, bus, subway, boat, ...) are variables, providing information about each month’s average ridership. The months themselves are observations (Currently, months are listed as variable names; rather, they should be intheir own column). You can tell which is which because as you go through time, the month changes, but themodes of transport offered by the T do not. As is customary, you want to represent variables in columns ratherthan rows.\n",
    "\n",
    "1.Pivot the rows and columns of the mbta data so that all columns are variables of the data. This shouldresult in 3 columns - mode, date, and number of riders in thousands (thou_riders).\n",
    "2.What are the new dimensions of this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  mode     date  thou_riders\n",
      "0                 Boat  2007-01        4.000\n",
      "1                  Bus  2007-01      335.819\n",
      "2        Commuter Rail  2007-01      142.200\n",
      "3           Heavy Rail  2007-01      435.294\n",
      "4           Light Rail  2007-01      227.231\n",
      "..                 ...      ...          ...\n",
      "459         Heavy Rail  2011-10      554.932\n",
      "460         Light Rail  2011-10      237.768\n",
      "461        Private Bus  2011-10        2.967\n",
      "462               RIDE  2011-10        8.598\n",
      "463  Trackless Trolley  2011-10       12.297\n",
      "\n",
      "[464 rows x 3 columns]\n",
      "The new dimensions of the data are (464, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mbta_2_drop\n",
    "mbta_result = mbta_2_drop.melt(\n",
    "    id_vars='mode',\n",
    "    var_name='date',\n",
    "    value_name='thou_riders'\n",
    ")\n",
    "print(mbta_result)\n",
    "print(\"The new dimensions of the data are\" , mbta_result.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Your data set is already looking much better! Your boss saw what a great job you’re doing and now wants youto do an analysis of the T’s ridership during certain months across all years. Your data set has months in it, so that analysis will be a piece of cake. There’s only one small problem: if you want to look at ridership on the Tduring every January (for example), the month and year are together in the same column, which makes it a littletricky. You’ll need to separate the month column into distinct month and year columns to make life easier.\n",
    "1.\n",
    "Split the month column of mbta at the dash and create a new month column with only the month and ayear column with only the year.\n",
    "2.\n",
    "View the head of this new mbta data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The head of this new data set\n",
      "            mode     date  thou_riders  year month\n",
      "0           Boat  2007-01        4.000  2007    01\n",
      "1            Bus  2007-01      335.819  2007    01\n",
      "2  Commuter Rail  2007-01      142.200  2007    01\n",
      "3     Heavy Rail  2007-01      435.294  2007    01\n",
      "4     Light Rail  2007-01      227.231  2007    01\n"
     ]
    }
   ],
   "source": [
    "type(mbta_result)\n",
    "\n",
    "mbta_result[['year', 'month']] = mbta_result['date'].str.split(\"-\", expand = True)\n",
    "mbta_result[['year','month']] = mbta_result.date.str.split(\"-\",expand=True)\n",
    "mbta_result.drop(columns=['date'])\n",
    "print(\"The head of this new data set\")\n",
    "print(mbta_result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Every month, average weekday commuter boat ridership was around 4,000. Then, one month it jumped to40,000 without warning? Unless the Olympics were happening in Boston that month (they weren’t), this value iscertainly an error. You can assume that whoever was entering the data that month accidentally typed 40 insteadof 4.\n",
    "1.\n",
    "Locate the row and column of the incorrect value.\n",
    "2.\n",
    "Replace the incorrect value with 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The located value\n",
      "    mode     date  thou_riders  year month\n",
      "16  Boat  2007-03         40.0  2007    03\n",
      "                  mode     date  thou_riders  year month\n",
      "0                 Boat  2007-01        4.000  2007    01\n",
      "1                  Bus  2007-01      335.819  2007    01\n",
      "2        Commuter Rail  2007-01      142.200  2007    01\n",
      "3           Heavy Rail  2007-01      435.294  2007    01\n",
      "4           Light Rail  2007-01      227.231  2007    01\n",
      "..                 ...      ...          ...   ...   ...\n",
      "459         Heavy Rail  2011-10      554.932  2011    10\n",
      "460         Light Rail  2011-10      237.768  2011    10\n",
      "461        Private Bus  2011-10        2.967  2011    10\n",
      "462               RIDE  2011-10        8.598  2011    10\n",
      "463  Trackless Trolley  2011-10       12.297  2011    10\n",
      "\n",
      "[464 rows x 5 columns]\n",
      "The replaced value\n",
      "    mode     date  thou_riders  year month\n",
      "0   Boat  2007-01          4.0  2007    01\n",
      "16  Boat  2007-03          4.0  2007    03\n",
      "The new data frame with replaced value for 'thou_riders is                   mode     date  thou_riders  year month\n",
      "0                 Boat  2007-01        4.000  2007    01\n",
      "1                  Bus  2007-01      335.819  2007    01\n",
      "2        Commuter Rail  2007-01      142.200  2007    01\n",
      "3           Heavy Rail  2007-01      435.294  2007    01\n",
      "4           Light Rail  2007-01      227.231  2007    01\n",
      "..                 ...      ...          ...   ...   ...\n",
      "459         Heavy Rail  2011-10      554.932  2011    10\n",
      "460         Light Rail  2011-10      237.768  2011    10\n",
      "461        Private Bus  2011-10        2.967  2011    10\n",
      "462               RIDE  2011-10        8.598  2011    10\n",
      "463  Trackless Trolley  2011-10       12.297  2011    10\n",
      "\n",
      "[464 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "condi_a = mbta_result.loc[mbta_result['thou_riders'] == 40 ]\n",
    "print(\"The located value\")\n",
    "print(cond_a)\n",
    "\n",
    "data_new2 = mbta_result.copy()                                 # Create copy of DataFrame\n",
    "data_new2['thou_riders'] = data_new2['thou_riders'].replace(40, 4)  # Replace values in DataFrame\n",
    "print(data_new2) \n",
    "\n",
    "new_mbta_df = data_new2.loc[data_new2['thou_riders'] == 4 ]\n",
    "print(\"The replaced value\")\n",
    "print(new_mbta_df)\n",
    "print(\"The new data frame with replaced value for 'thou_riders is\" , data_new2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Congrats, your data is now clean and ready for analysis.\n",
    "1.Compute the mean ridership per mode.\n",
    "2.Compute the mean ridership per mode for the month of January.\n",
    "3.Which year had the greatest ridership for the boat mode?\n",
    "4.On average, which month experiences the greatest number of passengers on the Heavy Rail mode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats, your data is now clean and ready for analysis.\n",
      "the mean ridership per mode\n",
      "                  thou_riders\n",
      "                         mean\n",
      "mode                         \n",
      "Boat                     4.45\n",
      "Bus                    358.59\n",
      "Commuter Rail          137.35\n",
      "Heavy Rail             489.29\n",
      "Light Rail             232.99\n",
      "Private Bus              3.35\n",
      "RIDE                     6.60\n",
      "Trackless Trolley       12.13\n",
      "The mean ridership per mode for the month of January\n",
      "                  thou_riders\n",
      "                         mean\n",
      "mode                         \n",
      "Boat                     3.31\n",
      "Bus                    342.37\n",
      "Commuter Rail          137.02\n",
      "Heavy Rail             460.28\n",
      "Light Rail             217.39\n",
      "Private Bus              3.47\n",
      "RIDE                     5.94\n",
      "Trackless Trolley       12.47\n",
      "the greatest ridership for the boat mode is\n",
      "      thou_riders\n",
      "year             \n",
      "2007         57.1\n",
      "2008         50.3\n",
      "2009         50.6\n",
      "2010         52.9\n",
      "2011         47.1\n",
      "The greatest number of passengers on the Heavy Rail mode\n",
      "       thou_riders\n",
      "month             \n",
      "09           518.0\n",
      "10           516.0\n",
      "04           502.0\n",
      "06           498.0\n",
      "07           495.0\n",
      "05           495.0\n",
      "11           490.0\n",
      "03           484.0\n",
      "02           481.0\n",
      "08           477.0\n",
      "01           460.0\n",
      "12           447.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Congrats, your data is now clean and ready for analysis.\")\n",
    "data_new2\n",
    "mean_rider_ship = data_new2.groupby('mode').agg({'thou_riders' : ['mean']})\n",
    "mean_rider_ship_rd = mean_rider_ship.round(2)\n",
    "print('the mean ridership per mode')\n",
    "print(mean_rider_ship_rd)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#2\n",
    "\n",
    "data_new_3 = data_new2.loc[data_new2['month'] == '01']\n",
    "data_new_3\n",
    "\n",
    "avg1_rider_per_mode_jan = data_new_3.groupby('mode').agg({'thou_riders' : ['mean']})\n",
    "\n",
    "avg1_rider_per_mode_jan_rd = avg1_rider_per_mode_jan.round(2)\n",
    "print(\"The mean ridership per mode for the month of January\")\n",
    "print(avg1_rider_per_mode_jan_rd)\n",
    "\n",
    "#3\n",
    "\n",
    "data_new_4 = data_new2.loc[data_new2['mode'] == 'Boat']\n",
    "data_new_4\n",
    "\n",
    "great_rider_boat_md = data_new_4.groupby('year').agg({'thou_riders': 'sum'})\n",
    "great_rider_boat_md\n",
    "print(\"the greatest ridership for the boat mode is\")\n",
    "\n",
    "great_rider_boat_md_rd = great_rider_boat_md.round(1)\n",
    "print(great_rider_boat_md_rd)\n",
    "\n",
    "#4\n",
    "\n",
    "data_new_5 = data_new2.loc[data_new2['mode'] == 'Heavy Rail']\n",
    "data_new_5\n",
    "\n",
    "great_nos_passenger = data_new_5.groupby('month').agg({'thou_riders': 'mean'}).nlargest(500, 'thou_riders')\n",
    "great_nos_passenger\n",
    "\n",
    "print('The greatest number of passengers on the Heavy Rail mode')\n",
    "\n",
    "great_nos_passenger_rd = great_nos_passenger.round(0)\n",
    "print(great_nos_passenger_rd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2\n",
    "For this module we will be using the\n",
    "completejourney_py\n",
    "data sets that you saw in the lesson 4b reading:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using the transactions and demographics data, how many of the 1,469,307 transactions do we have demographic information for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "household_id             828850\n",
       "store_id                 828850\n",
       "basket_id                828850\n",
       "product_id               828850\n",
       "quantity                 828850\n",
       "sales_value              828850\n",
       "retail_disc              828850\n",
       "coupon_disc              828850\n",
       "coupon_match_disc        828850\n",
       "week                     828850\n",
       "transaction_timestamp    828850\n",
       "age                      828850\n",
       "income                   828850\n",
       "home_ownership           607943\n",
       "marital_status           687418\n",
       "household_size           828850\n",
       "household_comp           828850\n",
       "kids_count               828850\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from completejourney_py import get_data\n",
    "\n",
    "cj_data = get_data()\n",
    "cj_data.keys()\n",
    "transactions = cj_data['transactions']\n",
    "demographics = cj_data['demographics']\n",
    "transactions.columns.intersection(demographics.columns)\n",
    "\n",
    "we_have_demo_info = transactions.merge(demographics)\n",
    "we_have_demo_info.shape\n",
    "we_have_demo_info.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using the transactions and demographics data, compute the total sales_value by age category toidentify which age group generates the most sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total sales_value by age category to identify which age group generates the most sales.      age  sales_value\n",
      "3  45-54    971822.19\n",
      "2  35-44    724357.40\n",
      "1  25-34    453372.46\n",
      "5    65+    176600.84\n",
      "4  55-64    173153.70\n",
      "0  19-24    125673.05\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "new_df= pd.merge(transactions,demographics,on='household_id')\n",
    "new_df.columns\n",
    "\n",
    "total_sales_by_age= new_df.groupby('age', as_index=False).agg({'sales_value': 'sum'}).nlargest(8, 'sales_value')\n",
    "\n",
    "print('the total sales_value by age category to identify which age group generates the most sales.' , total_sales_by_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Identify all different products that contain “pizza” in their product_type description. Which of these products produces the greatest amount of total sales (compute total sales by product ID and product type)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The products produces the greatest amount of total sales (compute total sales by product ID and product type) are                                sales_value\n",
      "product_id product_type                   \n",
      "944139     PIZZA/TRADITIONAL       1344.50\n",
      "906838     PIZZA/PREMIUM           1263.78\n",
      "12648296   PIZZA/TRADITIONAL       1230.36\n",
      "969568     PIZZA/TRADITIONAL       1125.18\n",
      "925626     PIZZA/ECONOMY           1017.60\n",
      "...                                    ...\n",
      "610240     PIZZA/ECONOMY              0.99\n",
      "1231548    PIZZA/ECONOMY              0.99\n",
      "2063332    PIZZA/ECONOMY              0.67\n",
      "6876820    PIZZA/ECONOMY              0.59\n",
      "10341942   QSR FD: COLD PIZZA         0.00\n",
      "\n",
      "[394 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "products = cj_data['products']\n",
    "\n",
    "\n",
    "\n",
    "new_df_2= pd.merge(transactions,products,on='product_id')\n",
    "new_df_2.columns\n",
    "\n",
    "\n",
    "p_prod_ty = new_df_2[new_df_2.product_type.str.contains('pizza' , case=False, na=False)]\n",
    "p_prod_ty_1 = p_prod_ty.groupby(['product_id', 'product_type']).agg({'sales_value': 'sum'}).nlargest(400, 'sales_value')\n",
    "p_prod_ty_1\n",
    "\n",
    "print('The products produces the greatest amount of total sales (compute total sales by product ID and product type) are' , p_prod_ty_1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Identify all products that are categorized (product_category) as “pizza” but are considered a “snack” or “appetizer” (viaproduct_type). Which of these products (product_id) have the most number of sales(measured by quantity)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            quantity\n",
      "product_id          \n",
      "845193           847\n",
      "890695           718\n",
      "907631           632\n",
      "856252           479\n",
      "1103105          276\n",
      "...              ...\n",
      "5712355            1\n",
      "6553886            1\n",
      "6563745            1\n",
      "6570944            1\n",
      "15837271           1\n",
      "\n",
      "[188 rows x 1 columns]\n",
      "The next result\n",
      "The products with most number of sales measured by quantity are              quantity\n",
      "product_id          \n",
      "845193           847\n",
      "890695           718\n",
      "907631           632\n",
      "856252           479\n",
      "1103105          276\n",
      "...              ...\n",
      "5712355            1\n",
      "6553886            1\n",
      "6563745            1\n",
      "6570944            1\n",
      "15837271           1\n",
      "\n",
      "[188 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "products = cj_data['products']\n",
    "new_df_3= pd.merge(transactions,products,on='product_id')\n",
    "new_df_3.columns\n",
    "\n",
    "p_categ = new_df_3[new_df_3.product_category.str.contains('pizza' , case=False, na=False)]\n",
    "p_categ\n",
    "\n",
    "\n",
    "p_type_1 = p_categ[p_categ.product_type.str.contains('snacks|appetizers', case=False, na=False)]\n",
    "print(\"Products that are categorized as 'pizza' and the product_type as 'snack | appetizer' are \",p_type_1)\n",
    "\n",
    "\n",
    "more_sale = p_type_1.groupby('product_id').agg({'quantity': 'sum'}).nlargest(400, 'quantity')\n",
    "\n",
    "print(more_sale)\n",
    "\n",
    "print(\"The next result\")\n",
    "\n",
    "print(\"The products with most number of sales measured by quantity are \", more_sale)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Identify all products that contain “peanut butter” in their product_type. How many unique products does thisresult in? For these products, compute the total sales_value\n",
    "by month based on the transaction_timestamp. Which month produces the most sales value for these products?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique produccts with 'PEANUT BUTTER' in product type are       household_id  store_id    basket_id  product_id  quantity  sales_value  \\\n",
      "0             1333       317  31198475743     1051211         1         2.39   \n",
      "1             1947     32004  31198517101     1051211         1         2.39   \n",
      "2             2040       408  31198536427     1051211         1         2.39   \n",
      "3              358       439  31225666749     1051211         1         2.39   \n",
      "4              354       346  31225866161     1051211         1         2.39   \n",
      "...            ...       ...          ...         ...       ...          ...   \n",
      "3815          1966       333  40941640672      889041         1         6.59   \n",
      "3816          1814      3274  40621530774       43065         1         1.50   \n",
      "3817          1453       317  40702975960      860460         1         2.79   \n",
      "3818          1675      3235  41011571443    14029132         1         2.59   \n",
      "3819          1001      3131  41429516853      198651         1         2.39   \n",
      "\n",
      "      retail_disc  coupon_disc  coupon_match_disc  week transaction_timestamp  \\\n",
      "0            0.00          0.0                0.0     1   2017-01-01 18:33:25   \n",
      "1            0.00          0.0                0.0     1   2017-01-01 22:01:00   \n",
      "2            0.00          0.0                0.0     1   2017-01-02 03:23:16   \n",
      "3            0.00          0.0                0.0     2   2017-01-02 19:31:29   \n",
      "4            0.00          0.0                0.0     2   2017-01-02 19:47:36   \n",
      "...           ...          ...                ...   ...                   ...   \n",
      "3815         2.40          0.0                0.0    49   2017-12-01 15:55:43   \n",
      "3816         0.00          0.0                0.0    45   2017-11-05 20:24:44   \n",
      "3817         0.00          0.0                0.0    46   2017-11-12 18:11:36   \n",
      "3818         0.56          0.0                0.0    49   2017-12-03 22:06:25   \n",
      "3819         0.00          0.0                0.0    53   2017-12-28 18:44:31   \n",
      "\n",
      "      manufacturer_id department     brand    product_category  \\\n",
      "0                1266    GROCERY  National  PNT BTR/JELLY/JAMS   \n",
      "1                1266    GROCERY  National  PNT BTR/JELLY/JAMS   \n",
      "2                1266    GROCERY  National  PNT BTR/JELLY/JAMS   \n",
      "3                1266    GROCERY  National  PNT BTR/JELLY/JAMS   \n",
      "4                1266    GROCERY  National  PNT BTR/JELLY/JAMS   \n",
      "...               ...        ...       ...                 ...   \n",
      "3815             2438  NUTRITION  National          CONDIMENTS   \n",
      "3816               69    GROCERY   Private  PNT BTR/JELLY/JAMS   \n",
      "3817             1266    GROCERY  National  PNT BTR/JELLY/JAMS   \n",
      "3818              683    GROCERY  National  PNT BTR/JELLY/JAMS   \n",
      "3819             1266    GROCERY  National  PNT BTR/JELLY/JAMS   \n",
      "\n",
      "                   product_type package_size  \n",
      "0                 PEANUT BUTTER        18 OZ  \n",
      "1                 PEANUT BUTTER        18 OZ  \n",
      "2                 PEANUT BUTTER        18 OZ  \n",
      "3                 PEANUT BUTTER        18 OZ  \n",
      "4                 PEANUT BUTTER        18 OZ  \n",
      "...                         ...          ...  \n",
      "3815  NUT BUTTERS/PEANUT BUTTER        11 OZ  \n",
      "3816              PEANUT BUTTER   ALL  18 OZ  \n",
      "3817              PEANUT BUTTER        16 OZ  \n",
      "3818              PEANUT BUTTER        16 OZ  \n",
      "3819              PEANUT BUTTER        18 OZ  \n",
      "\n",
      "[3820 rows x 17 columns]\n",
      "the total sales value is computed by month based on transaction_timestamp                        sales_value\n",
      "transaction_timestamp             \n",
      "2017-12-31                 1019.08\n",
      "2017-09-30                  905.36\n",
      "2017-08-31                  895.10\n",
      "2017-11-30                  882.32\n",
      "2017-10-31                  878.89\n",
      "The month with the most sales value is 'December'                        sales_value\n",
      "transaction_timestamp             \n",
      "2017-12-31                 1019.08\n",
      "2017-09-30                  905.36\n",
      "2017-08-31                  895.10\n",
      "2017-11-30                  882.32\n",
      "2017-10-31                  878.89\n"
     ]
    }
   ],
   "source": [
    "products = cj_data['products']\n",
    "\n",
    "new_df_5= pd.merge(transactions,products,on='product_id')\n",
    "new_df_5.columns\n",
    "\n",
    "# ----\n",
    "peanut_count_1 = products[products.product_type.str.contains('peanut butter' , case=False, na=False)]\n",
    "peanut_count_1\n",
    "type(peanut_count_1)\n",
    "\n",
    "\n",
    "new_df_6_1 = pd.merge(transactions,peanut_count_1,on='product_id')\n",
    "print(\"The unique produccts with 'PEANUT BUTTER' in product type are\",new_df_6_1)\n",
    "\n",
    "# ---\n",
    "\n",
    "p_prod_ty_5 = new_df_5[new_df_5.product_type.str.contains('peanut butter' , case=False, na=False)]\n",
    "p_prod_ty_5\n",
    "\n",
    "p_prod_ty_6 = p_prod_ty_5.groupby(pd.Grouper(key='transaction_timestamp', axis=0,freq='M')).agg({'sales_value': 'sum'}).nlargest(5, 'sales_value')\n",
    "\n",
    "print(\"the total sales value is computed by month based on transaction_timestamp\",p_prod_ty_6)\n",
    "\n",
    "print(\"The month with the most sales value is 'December'\",p_prod_ty_6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7af7df4901773a0e355da496bf365ae011b1b331a57bbc9908dae1ee21823d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
